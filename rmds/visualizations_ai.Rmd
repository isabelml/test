---
title: "Visualizations and text analysis"
subtitle: "Master´s Thesis. Computational Social Sciences"
author: "Isabel Molero López"
date: "Sys.Date()"
output: html_notebook
  toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction

This document shows the code needed to create a line graph showing the evolution 
of the articles published by two digital newspapers (El Mundo and elDiario.es) on 
artificial intelligence from 2014 to 2023. In addition, a text analysis is also 
performed for which several visualizations are included. The code needed to 
extract and clean the data used in this file is located in the same GitHub 
repository.

# 2. Load libraries

```{r message=FALSE, warning=FALSE}
# General libraries
library(tidyverse)

# To perform text analysis:
library(stopwords)
library(tidytext)
library(textdata)
library(syuzhet)

# To make plots:
library(ggplot2)
library(scales)
library(ggpubr)
library(RColorBrewer)
library(RColorBrewer)
library(wordcloud)
library(reshape2)
library(forcats)
library(igraph)
```

# 3. Read data
```{r message=FALSE}
eldiario_all <- read_csv("../data/eldiario_all.csv")
elmundo_all <- read_csv("../data/elmundo_all.csv")
```

# 4. Line graph: number of news articles published over time

## Create a column with the month and year

```{r}
eldiario_all$year_month <- format(eldiario_all$date, "%Y-%m")
elmundo_all$year_month <- format(elmundo_all$date, "%Y-%m")
```

## Filter only the articles which contains in the text the words "inteligencia artificial"

```{r}
eldiario_all_filter <- eldiario_all %>% 
  filter(str_detect(text, regex("inteligencia artificial", ignore_case = TRUE)))

elmundo_all_filter <- elmundo_all %>% 
  filter(str_detect(text, regex("inteligencia artificial", ignore_case = TRUE))) %>% 
  filter(!is.na(date)) # remove NAs in the column "date"
```

## Clean dates

Now I am going to set the same minimum and maximum date of publication of 
articles for both data sets:

```{r}
# Check max and min dates for both datasets:
min(eldiario_all_filter$date)
min(elmundo_all_filter$date)

max(eldiario_all_filter$date)
max(elmundo_all_filter$date)
```

```{r}
eldiario_all_filter <- eldiario_all_filter %>% 
  filter(date >= "2014-07-28") %>% 
  filter(date <= "2023-04-30")

elmundo_all_filter <- elmundo_all_filter %>% 
  filter(date <= "2023-04-30")
```

## Join both dataframes

```{r}
final_df <- eldiario_all_filter %>% 
  mutate(newspaper = "elDiario.es") %>% 
  bind_rows(elmundo_all_filter %>%
              mutate(newspaper = "elmundo.es")) 

# Remove some rows that correspond to advertisements:
final_df <- final_df[!grepl("^https://www.elmundo.es/ofertas-regalos", final_df$link), ]
```

### Table with the number of articles published by year

```{r}
# Sample of news articles by year and newspaper

# eldiario.es:
final_df %>% 
  filter(newspaper == "elDiario.es") %>% 
  mutate(year = year(date)) %>% 
  count(year)

# elmundo.es:
final_df %>% 
  filter(newspaper == "elmundo.es") %>% 
  mutate(year = year(date)) %>% 
  count(year)
```


## Plot

### Create new column with the quarterly period

```{r}
library(RColorBrewer)

# New date column only with year and month:
final_df <- final_df %>%
  mutate(year_month = ym(year_month))

# New column with the four-month period:
final_df <- final_df %>%
  mutate(q_year = case_when(
    month(year_month) %in% c(1, 2, 3, 4) ~ paste0("Q1 ", format(year_month, "%Y")),
    month(year_month) %in% c(5, 6, 7, 8) ~ paste0("Q2 ", format(year_month, "%Y")),
    month(year_month) %in% c(9, 10, 11, 12) ~ paste0("Q3 ", format(year_month, "%Y"))
  ))

# Order the year and month:
final_df <- final_df %>%
  mutate(q_year = reorder(q_year, year_month))
```

### Create the plot with ggplot2

```{r fig.height=4.5, fig.width=6.7, warning=FALSE}
ggplot(final_df, aes(x = q_year, group = newspaper, color = newspaper)) +
  geom_line(stat = "count", position = "identity", linewidth = 0.8) +
  labs(title = "Number of articles published per quarter and year",
       subtitle = "2014-2023",
       caption = " Note: the y-axis shows the absolute frequency of news published on the website of each newspaper containing the  \n words 'inteligencia artificial'") +
  scale_x_discrete(breaks = final_df$q_year[grepl("Q1", final_df$q_year)]) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_blank(),
        legend.text = element_text(family = "Times New Roman", size = 11),
        legend.position = "top",
        legend.justification = "left",
        legend.box.margin = margin(l = -12),
        legend.title = element_blank(),
        plot.title = element_text(family = "Times New Roman", size = 14, 
                                  face = "bold"),
        plot.subtitle = element_text(family = "Times New Roman", size = 12),
        plot.caption = element_text(family = "Times New Roman", size = 10,
                                    hjust = 0, margin = margin(t = 15)),
        plot.caption.position = "plot") +
  scale_color_brewer(palette = "Dark2") +
  # geom_vline(xintercept = "Q1 2022", linetype = "dashed", color = "black")
  geom_segment(aes(x = "Q1 2022", xend = "Q1 2022", y = 0, yend = 110), 
               size = 0.4, linetype = "dashed", colour = "black", alpha = 0.02) +
  geom_segment(aes(x = "Q1 2021", xend = "Q1 2021", y = 0, yend = 125), 
               size = 0.3, linetype = "dashed", colour = "black", alpha = 0.02) +
  geom_segment(aes(x = "Q2 2022", xend = "Q2 2022", y = 0, yend = 130), 
               size = 0.3, linetype = "dashed", colour = "black", alpha = 0.02) +
  geom_segment(aes(x = "Q3 2015", xend = "Q3 2015", y = 0, yend = 125), 
               size = 0.3, linetype = "dashed", colour = "black", alpha = 0.02) +
  geom_segment(aes(x = "Q3 2014", xend = "Q3 2014", y = 0, yend = 125), 
               size = 0.3, linetype = "dashed", colour = "black", alpha = 0.02) +
  geom_segment(aes(x = "Q1 2017", xend = "Q1 2017", y = 0, yend = 125), 
               size = 0.3, linetype = "dashed", colour = "black", alpha = 0.02) +
  annotate("text", x = "Q1 2022", y = 120, label = "ChatGPT   ", 
           family = "Times New Roman") +
  annotate("text", x = "Q1 2021", y = 135, label = "DALL-E", 
           family = "Times New Roman") +
  annotate("text", x = "Q2 2022", y = 140, label = "Midjourney", 
           family = "Times New Roman") +
  annotate("text", x = "Q3 2015", y = 140, label = "AlphaGo \nMETA AI", 
           family = "Times New Roman") +
  annotate("text", x = "Q3 2014", y = 135, label = "Cortana", 
           family = "Times New Roman") +
    annotate("text", x = "Q1 2017", y = 135, label = "Libratus", 
           family = "Times New Roman")
```

```{r}
# Frequency table by newspaper and quarter
final_df %>% 
  count(newspaper, q_year) 
```

# Text-analysis visualizations

## Take a look to the text

```{r}
head(final_df %>% filter(newspaper == "elDiario.es") %>%  select(text))
```

```{r}
head(final_df %>% filter(newspaper == "elmundo.es") %>%  select(text))
```

## Remove first line in elDiario.es "EFE" news

E.g: "\n Caracas, 27 abr (EFE).-"

```{r}
head(final_df %>% filter(newspaper == "elDiario.es") %>%  select(text))
```


```{r}
final_df_test <- final_df %>% 
  filter(!str_detect(text, regex("(EFE).", ignore_case = FALSE)))

final_df_test2 <- final_df %>% 
  filter(str_detect(text, regex("(EFE).", ignore_case = FALSE))) %>% 
  mutate(text = sub("^[^.]*\\.", "", text))

# Join the dataframes again:
final_df <- final_df_test %>% 
  bind_rows(final_df_test2)

rm(final_df_test, final_df_test2)

head(final_df %>% filter(newspaper == "elDiario.es") %>%  select(text))
```

## Let's break the text into words

```{r}
tokens <- final_df %>%
  unnest_tokens(word, text, drop = F, to_lower = T) %>% 
  # I remove the date column:
  select(word, date, newspaper, q_year)

tokens
```

## Filter stop words

Then I remove the words that are not useful for the analysis with the 
"stopwords()" library in Spanish. In addition, I have removed other words that 
were not included in this vector but which I consider have no value for the 
analysis.

```{r}
spanish_stopwords <- stopwords("es")
additional_stopwords <- c("si","ser","inteligencia","artificial","además", "ia", "así", "según",
                          "cada", "dos", "descuento")
spanish_stopwords <- c(spanish_stopwords, additional_stopwords)

tokens <- tokens %>% 
  anti_join(data.frame(word = spanish_stopwords), by = "word") %>% 
  filter(word != "si" & word != "ser" & word != "inteligencia" & word != "artificial" & word != "además")

tokens
```

## Counting word frequencies

```{r}
tokens %>%
  count(word, sort = TRUE) 
```

### Visualize word frequencies

The following graphs shows the frequency of the most repeated words.

#### elDiario.es

```{r}
tokens %>%
  filter(newspaper == "elDiario.es") %>% 
  count(word, sort = TRUE) %>%
  #only words mentioned over 650 times in the speeches
  filter(n > 650) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col(fill = "#EBEF95") +
  labs(title = "Word frequency in artificial intelligence articles published by eldiario.es",
       subtitle = "2014-2023",
       caption = "Source: elDiario.es",
    y = NULL,
    x = NULL) +
  theme_minimal() +
  theme(plot.title.position = "plot",
        plot.title = element_text(family = "Times New Roman", size = 14, face = "bold",
                                  margin = margin(0, 0, 0, 0)),
        plot.subtitle = element_text(family = "Times New Roman", size = 12,
                                     margin = margin(5, 0, 20, 0),),
        axis.text.y = element_text(hjust = 0, family = "Times New Roman", size = 11,
                                   colour = "black"))
```

#### El Mundo

```{r}
tokens %>%
  filter(newspaper == "elmundo.es") %>% 
  count(word, sort = TRUE) %>%
  #only words mentioned over 925 times in the speeches
  filter(n > 925) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col(fill = "#EFB495") +
  labs(title = "Word frequency in artificial intelligence articles published by elmundo.es",
       subtitle = "2014-2023",
       caption = "Source: elmundo.es",
    y = NULL,
    x = NULL) +
  theme_minimal() +
  theme(plot.title.position = "plot",
        plot.title = element_text(family = "Times New Roman", size = 14, face = "bold",
                                  margin = margin(0, 0, 0, 0)),
        plot.subtitle = element_text(family = "Times New Roman", size = 12,
                                     margin = margin(5, 0, 20, 0),),
        axis.text.y = element_text(hjust = 0, family = "Times New Roman", size = 11,
                                   colour = "black"))
```

### Comparing word frequencies across the two different newspapers

```{r}
library(tidyr)

frequency <- tokens %>% 
  mutate(word = str_extract(word, "\\b\\p{L}+\\b")) %>%
  count(newspaper, word) %>%
  group_by(newspaper) %>%
  mutate(proportion = n / sum(n)) %>% 
  filter(n > 15) %>% 
  select(-n) %>% 
  pivot_wider(names_from = newspaper, values_from = proportion) %>%
  pivot_longer(`elDiario.es`,
               names_to = "newspaper", values_to = "proportion")

frequency
```

```{r warning=FALSE}
# Create a plot comparing word frequencies:
frequency %>% 
  ggplot(aes(x = proportion, y = `elmundo.es`, 
                      color = abs(`elmundo.es` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 0.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 0.5) +
  theme_minimal() +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(y = "elmundo.es", x = "elDiario.es") 
```

### Calculating correlation

Now I am going to quantify how similar are both datasets (elDiario.es and El 
Mundo) with the Pearson correlation coefficient. 

The correlation coefficient between the texts of both newspapers shows that they
are very similar. The coefficient gives 0.97, which indicates that they are not 
identical but semantically very similar. 

```{r}
frequency <- tokens %>% 
  mutate(word = str_extract(word, "\\b\\p{L}+\\b")) %>%
  count(newspaper, word) %>%
  group_by(newspaper) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = newspaper, values_from = proportion) %>%
  pivot_longer(`elDiario.es`,
               names_to = "newspaper", values_to = "proportion")

cor.test(data = frequency[frequency$newspaper == "elDiario.es",],
         ~ proportion + `elmundo.es`)
```

## Sentiment analysis

In this section I will conduct a sentiment analysis of the news articles. 

To do the sentiment analysis I first download the NRC lexicon in Spanish from the
following website: https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

Once I extract the excel I need into my folder I read it, remove the column of 
English words that I don't need, pivot it to a long format and I am left with one 
column with the word and another with the sentiment associated with that word.

```{r}
sentiment_spanish_nrc <- readxl::read_xlsx("../data/nrc_spanish.xlsx") %>% 
  select(-'English Word') %>% 
  pivot_longer(anger:trust, names_to = "sentiment") %>% 
  filter(value == 1) %>% 
  select(-value) %>% 
  rename(word = 'Spanish Word') 

# To view the sentiments that include NRC lexicon: 
unique(sentiment_spanish_nrc$sentiment) 
```

### Visualization of the eight sentiments included in NRC lexicon

```{r fig.width= 5.5, fig.height=6, warning=FALSE}
my_palette <- brewer.pal(8, "Set1")

# eldiario.es:
sentiment_eldiario <- tokens %>%
  filter(newspaper == "elDiario.es") %>% 
  inner_join(sentiment_spanish_nrc, by = "word") %>%
  count(sentiment, sort = T) %>% 
  filter(sentiment != "positive" & sentiment != "negative") %>% 
  mutate(sentiment = reorder(sentiment, desc(n))) %>%
  ggplot(aes(sentiment, n, fill = sentiment)) +
  geom_col() +
  theme_minimal() +
  labs(title = "Sentiment analysis of elDiario.es artificial intelligence articles",
       subtitle = "Frequency of the eight sentiments") +
  theme(legend.position = "none",
        axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        plot.title = element_text(face = "bold", size = 11, hjust = 0.45),
        plot.subtitle = element_text(size = 10, hjust = 0.49, margin = margin(b = 30)),
        axis.text.x = element_text(size = 8.5, face = "bold", color = "black")) +
  scale_fill_manual(values = my_palette)

# elmundo.es:
sentiment_elmundo <- tokens %>%
  filter(newspaper == "elmundo.es") %>% 
  inner_join(sentiment_spanish_nrc, by = "word") %>%
  count(sentiment, sort = T) %>% 
  filter(sentiment != "positive" & sentiment != "negative") %>% 
  mutate(sentiment = reorder(sentiment, desc(n))) %>%
  ggplot(aes(sentiment, n, fill = sentiment)) +
  geom_col() +
  theme_minimal() +
  labs(title = "Sentiment analysis of El Mundo artificial intelligence articles",
       subtitle = "Frequency of the eight sentiments") +
  theme(legend.position = "none",
        axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        plot.title = element_text(face = "bold", size = 11, hjust = 0.45),
        plot.subtitle = element_text(size = 10, hjust = 0.49, margin = margin(b = 30)),
        axis.text.x = element_text(size = 8.5, face = "bold", color = "black")) +
  scale_fill_manual(values = my_palette)

ggarrange(sentiment_eldiario, NULL, sentiment_elmundo,
  nrow = 3, heights = c(1, 0.2, 1))
  
```

### Fear words in the articles: NRC lexicon

I want to analyze the use of words of fear used in the articles.

First I filter in the dataframe created before with the NRC lexicon the words 
classified with the sentiment "fear".

```{r}
fear_words <- sentiment_spanish_nrc %>% 
  filter(sentiment == "fear")

# Visualize the new dataframe:
head(fear_words)
```

Then I put it together with the dataframe of the tokenized news articles and count 
the frequency of each word. In the following dataframe they are sorted from most to 
least frequent.

```{r warning=FALSE}
np_fear_words <- tokens %>%
    inner_join(fear_words, by = "word") %>%
    count(word, sort = TRUE)

head(np_fear_words)
```

### Visualization of fear words comparing elDiario.es and elmundo.es

```{r fig.width=8, warning=FALSE}
exclude_words <- c("caso", "grupo", "embargo", "contenido", "peso", "llevar", "granada")

plot_eldiario <- tokens %>%
  filter(newspaper == "elDiario.es") %>%
    filter(!word %in% exclude_words) %>%
    inner_join(fear_words, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(word_dif = case_when(word %in% c("peligro", "consejero", "defensa") ~ T,
                                           T ~ F)) %>% 
    filter(n > 100) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(x = n, y = word, fill = word_dif)) +
    geom_col() +
    labs(y = NULL,
         x = NULL,
         title = "Frequency of fear words",
         subtitle = "elDiario.es artificial intelligence articles 2014-2023") +
  theme_minimal()+
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.3),
        plot.subtitle = element_text(size = 9, hjust = 0.3),
        axis.text.y = element_text(hjust = 0, size = 9),
        legend.position = "none") +
  scale_fill_manual(values = c("#FFEC59", "#FC6238"))



plot_elmundo <- tokens %>%
  filter(newspaper == "elmundo.es") %>% 
  filter(!word %in% exclude_words) %>% 
    inner_join(fear_words, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(word_dif = case_when(word %in% c("falta", "dinero", "amenaza") ~ T,
                                           T ~ F)) %>% 
    filter(n > 125) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(x = n, y = word, fill = word_dif)) +
    geom_col() +
    labs(y = NULL,
         x = NULL,
         title = "Frequency of fear words",
         subtitle = "El Mundo artificial intelligence articles 2014-2023") +
  theme_minimal()+
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.3),
        plot.subtitle = element_text(size = 9, hjust = 0.3),
        axis.text.y = element_text(hjust = 0, size = 9),
        legend.position = "none") +
  scale_fill_manual(values = c("#FFEC59", "#FC6238"))

ggarrange(plot_eldiario, plot_elmundo)
```

### Wordclouds

Finally, I want to see the most repeated words in the articles in a word cloud.

```{r fig.height=9, fig.width=9}
colors = brewer.pal(8, 'Dark2')

tokens %>%
  filter(newspaper == "elDiario.es") %>% 
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, colors = colors))
```

```{r fig.height=9, fig.width=9}
tokens %>%
  filter(newspaper == "elmundo.es") %>% 
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, colors = colors))
```


