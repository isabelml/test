---
title: "Visualizations and text analysis"
author: "Isabel Molero López"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction

# 2. Load libraries
```{r message=FALSE, warning=FALSE}
# General libraries
library(tidyverse)

# To perform text analysis:
library(stopwords)
library(tidytext)
library(textdata)
library(syuzhet)

# To make plots:
library(ggplot2)
library(scales)
library(ggpubr)
library(RColorBrewer)
library(RColorBrewer)
library(wordcloud)
library(reshape2)
library(forcats)
library(igraph)
```

# 3. Read data
```{r message=FALSE}
eldiario_all <- read_csv("../data/eldiario_all.csv")
elmundo_all <- read_csv("../data/elmundo_all.csv")
```

# 4. Line graph: number of news articles published over time

## Create a column with the month and year

```{r}
eldiario_all$year_month <- format(eldiario_all$date, "%Y-%m")
elmundo_all$year_month <- format(elmundo_all$date, "%Y-%m")
```

## Filter only the articles which contains in the text the words "inteligencia artificial"

```{r}
eldiario_all_filter <- eldiario_all %>% 
  filter(str_detect(text, regex("inteligencia artificial", ignore_case = TRUE)))

elmundo_all_filter <- elmundo_all %>% 
  filter(str_detect(text, regex("inteligencia artificial", ignore_case = TRUE))) %>% 
  filter(!is.na(date)) # remove NAs in the column "date"
```

## Clean dates

Now I am going to set the same minimum and maximum date of publication of 
articles for both datasets:

```{r}
min(eldiario_all_filter$date)
min(elmundo_all_filter$date)

max(eldiario_all_filter$date)
max(elmundo_all_filter$date)
```
```{r}
eldiario_all_filter <- eldiario_all_filter %>% 
  filter(date >= "2014-07-28") %>% 
  filter(date <= "2023-04-30")

elmundo_all_filter <- elmundo_all_filter %>% 
  filter(date <= "2023-04-30")
```

## Join both dataframes

```{r}
final_df <- eldiario_all_filter %>% 
  mutate(newspaper = "elDiario.es") %>% 
  bind_rows(elmundo_all_filter %>%
              mutate(newspaper = "elmundo.es")) 
```


## Plot:

```{r}
library(RColorBrewer)

# Supongamos que tu dataframe se llama 'final_df'
# Asegúrate de que 'year_month' sea de tipo Date o POSIXct para poder realizar operaciones con fechas correctamente

# Crear una nueva columna 'year_month' en formato Date
final_df <- final_df %>%
  mutate(year_month = ym(year_month))

# Crear una nueva columna 'cuatrimestre_anio' basada en 'year_month'
final_df <- final_df %>%
  mutate(q_year = case_when(
    month(year_month) %in% c(1, 2, 3, 4) ~ paste0("Q1 ", format(year_month, "%Y")),
    month(year_month) %in% c(5, 6, 7, 8) ~ paste0("Q2 ", format(year_month, "%Y")),
    month(year_month) %in% c(9, 10, 11, 12) ~ paste0("Q3 ", format(year_month, "%Y"))
  ))

# Ordenar las etiquetas en orden cronológico
final_df <- final_df %>%
  mutate(q_year = reorder(q_year, year_month))
```


```{r}
# Crear el gráfico de líneas por cuatrimestres y periódicos
ggplot(final_df, aes(x = q_year, group = newspaper, color = newspaper)) +
  geom_line(stat = "count", position = "identity", size = 0.8) +
  labs(title = "Number of articles published per quarter and year",
       subtitle = "2014-2023") +
  scale_x_discrete(breaks = final_df$q_year[grepl("Q1", final_df$q_year)]) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_blank(),
        legend.text = element_text(family = "Times New Roman", size = 11),
        legend.position = "top",
        legend.justification = "left",
        legend.box.margin = margin(l = -12),
        legend.title = element_blank(),
        plot.title = element_text(family = "Times New Roman", size = 14, face = "bold"),
        plot.subtitle = element_text(family = "Times New Roman", size = 12)) +
  scale_color_brewer(palette = "Dark2") +
  # geom_vline(xintercept = "Q1 2022", linetype = "dashed", color = "black")
  geom_segment(aes(x = "Q1 2022", xend = "Q1 2022", y = 0, yend = 125), 
               size = 0.3, linetype = "dashed", colour = "black", alpha = 0.02) +
  geom_segment(aes(x = "Q1 2016", xend = "Q1 2016", y = 0, yend = 125), 
               size = 0.3, linetype = "dashed", colour = "black", alpha = 0.02) +
  annotate("text", x = "Q1 2022", y = 135, label = "Chat-GPT", family = "Times New Roman")
```

# Text-analysis visualizations

## Take a look to the text

```{r}
head(final_df %>% filter(newspaper == "elDiario.es") %>%  select(text))
```


```{r}
head(final_df %>% filter(newspaper == "elmundo.es") %>%  select(text))
```

## Remove first line in elDiario.es "EFE" news

```{r}
head(final_df %>% filter(newspaper == "elDiario.es") %>%  select(text))
```


```{r}
final_df_test <- final_df %>% 
  filter(!str_detect(text, regex("(EFE).", ignore_case = FALSE)))

final_df_test2 <- final_df %>% 
  filter(str_detect(text, regex("(EFE).", ignore_case = FALSE))) %>% 
  mutate(text = sub("^[^.]*\\.", "", text))

# Join the dataframes again
final_df <- final_df_test %>% 
  bind_rows(final_df_test2)

rm(final_df_test, final_df_test2)

head(final_df %>% filter(newspaper == "elDiario.es") %>%  select(text))
```

## Let's break the text into words

```{r}
tokens <- final_df %>%
  unnest_tokens(word, text, drop = F, to_lower = T) %>% 
  # I remove the date column:
  select(word, date, newspaper, q_year)

tokens
```

## Filter stop words

Then I eliminate the words that are not useful for the analysis with the 
"stopwords()" library in Spanish. In addition, I have the word "si" which was 
not included but which I consider has no value for the analysis.

```{r}
spanish_stopwords <- stopwords("es")

tokens <- tokens %>% 
  anti_join(data.frame(word = spanish_stopwords), by = "word") %>% 
  filter(word != "si")

tokens
```

## Counting word frequencies

```{r}
tokens %>%
  count(word, sort = TRUE) 
```

### Visualize word frequencies

The following graph shows the frequency of the most repeated words 

```{r}
tokens %>%
  count(word, sort = TRUE) %>%
  #only words mentioned over 1000 times in the speeches
  filter(n > 2000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col(fill = "#ff5252") +
  labs(title = "Word frequency in Pedro Sánchez's speeches in the year 2020",
       caption = "Source: La Moncloa, Intervenciones.",
    y = NULL,
    x = "Absolute frequency") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 10.5, hjust = 0.18,
                                  margin = margin(0, 0, 20, 0)),
        axis.text.y = element_text(hjust = 0))
```

### Comparing word frequencies across two different years

```{r}
library(tidyr)

frequency <- tokens %>% 
  mutate(word = str_extract(word, "\\b\\p{L}+\\b")) %>%
  count(newspaper, word) %>%
  group_by(newspaper) %>%
  mutate(proportion = n / sum(n)) %>% 
  filter(n > 15) %>% 
  select(-n) %>% 
  pivot_wider(names_from = newspaper, values_from = proportion) %>%
  pivot_longer(`elDiario.es`,
               names_to = "newspaper", values_to = "proportion")

frequency
```

```{r warning=FALSE}

frequency %>% 
  ggplot(aes(x = proportion, y = `elmundo.es`, 
                      color = abs(`elmundo.es` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 0.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 0.5) +
  theme_minimal() +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(y = "elmundo.es", x = "elDiario.es") 
```
